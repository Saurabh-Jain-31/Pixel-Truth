"""
PDF analysis service for detecting AI-generated content
"""
import os
import logging
import hashlib
import time
import re
from typing import Dict, Any, List
import fitz  # PyMuPDF
from textstat import flesch_reading_ease, flesch_kincaid_grade
import nltk
from collections import Counter

from app.models.analysis import PDFAnalysisResult

logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class PDFAnalysisService:
    def __init__(self):
        # AI-generated text indicators
        self.ai_indicators = [
            'as an ai', 'i am an ai', 'artificial intelligence',
            'generated by', 'created by ai', 'machine learning',
            'neural network', 'deep learning', 'chatgpt', 'gpt-',
            'claude', 'bard', 'copilot'
        ]
        
        # Common AI writing patterns
        self.ai_patterns = [
            r'\b(furthermore|moreover|additionally|consequently)\b',
            r'\b(it is important to note|it should be noted)\b',
            r'\b(in conclusion|to summarize|in summary)\b',
            r'\b(various|numerous|multiple|several)\b.*\b(aspects|factors|elements)\b'
        ]
    
    def get_file_hash(self, file_path: str) -> str:
        """Generate SHA256 hash of file"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    def extract_text_and_metadata(self, pdf_path: str) -> Dict[str, Any]:
        """Extract text and metadata from PDF"""
        try:
            doc = fitz.open(pdf_path)
            
            # Extract metadata
            metadata = doc.metadata
            
            # Extract text from all pages
            text_content = ""
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text_content += page.get_text()
            
            doc.close()
            
            return {
                'text': text_content,
                'metadata': metadata,
                'page_count': len(doc)
            }
            
        except Exception as e:
            logger.error(f"Error extracting PDF content: {e}")
            return {'text': '', 'metadata': {}, 'page_count': 0}
    
    def analyze_metadata_inconsistencies(self, metadata: Dict[str, Any]) -> List[str]:
        """Detect metadata inconsistencies that might indicate AI generation"""
        inconsistencies = []
        
        # Check for missing standard metadata
        standard_fields = ['title', 'author', 'creator', 'producer', 'creationDate', 'modDate']
        missing_fields = [field for field in standard_fields if not metadata.get(field)]
        
        if len(missing_fields) > 3:
            inconsistencies.append("Multiple standard metadata fields missing")
        
        # Check for suspicious creator/producer
        creator = metadata.get('creator', '').lower()
        producer = metadata.get('producer', '').lower()
        
        suspicious_creators = ['python', 'script', 'automated', 'bot', 'ai', 'generated']
        if any(sus in creator for sus in suspicious_creators):
            inconsistencies.append(f"Suspicious creator: {creator}")
        
        if any(sus in producer for sus in suspicious_creators):
            inconsistencies.append(f"Suspicious producer: {producer}")
        
        # Check for identical creation and modification dates (unusual for human-created docs)
        creation_date = metadata.get('creationDate')
        mod_date = metadata.get('modDate')
        if creation_date and mod_date and creation_date == mod_date:
            inconsistencies.append("Identical creation and modification dates")
        
        return inconsistencies
    
    def detect_ai_language_patterns(self, text: str) -> Dict[str, Any]:
        """Detect language patterns common in AI-generated text"""
        text_lower = text.lower()
        
        # Direct AI indicators
        ai_mentions = []
        for indicator in self.ai_indicators:
            if indicator in text_lower:
                ai_mentions.append(indicator)
        
        # Pattern matching
        pattern_matches = []
        for pattern in self.ai_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            if matches:
                pattern_matches.extend(matches)
        
        # Analyze sentence structure
        sentences = nltk.sent_tokenize(text)
        
        # Calculate average sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
        
        # Check for repetitive patterns
        words = text_lower.split()
        word_freq = Counter(words)
        most_common = word_freq.most_common(10)
        
        # Calculate readability scores
        try:
            flesch_score = flesch_reading_ease(text)
            fk_grade = flesch_kincaid_grade(text)
        except:
            flesch_score = 0
            fk_grade = 0
        
        return {
            'ai_mentions': ai_mentions,
            'pattern_matches': pattern_matches,
            'avg_sentence_length': avg_sentence_length,
            'most_common_words': most_common,
            'flesch_reading_ease': flesch_score,
            'flesch_kincaid_grade': fk_grade,
            'total_words': len(words),
            'total_sentences': len(sentences)
        }
    
    def calculate_ai_probability(self, text_analysis: Dict[str, Any], 
                               metadata_inconsistencies: List[str]) -> float:
        """Calculate probability that content is AI-generated"""
        score = 0.0
        
        # AI mentions (strong indicator)
        if text_analysis['ai_mentions']:
            score += 0.4
        
        # Pattern matches
        pattern_count = len(text_analysis['pattern_matches'])
        if pattern_count > 5:
            score += 0.3
        elif pattern_count > 2:
            score += 0.2
        
        # Metadata inconsistencies
        inconsistency_count = len(metadata_inconsistencies)
        if inconsistency_count > 2:
            score += 0.2
        elif inconsistency_count > 0:
            score += 0.1
        
        # Readability analysis
        flesch_score = text_analysis.get('flesch_reading_ease', 50)
        if 60 <= flesch_score <= 80:  # AI often produces "optimal" readability
            score += 0.1
        
        # Sentence length analysis
        avg_length = text_analysis.get('avg_sentence_length', 15)
        if 15 <= avg_length <= 25:  # AI tends to produce medium-length sentences
            score += 0.05
        
        # Word repetition analysis
        most_common = text_analysis.get('most_common_words', [])
        if most_common and most_common[0][1] > len(text_analysis.get('total_words', 0)) * 0.05:
            score += 0.05  # High word repetition
        
        return min(score, 1.0)
    
    def detect_suspicious_patterns(self, text: str, metadata: Dict[str, Any]) -> List[str]:
        """Detect various suspicious patterns"""
        patterns = []
        
        # Check for placeholder text
        placeholder_patterns = [
            r'\[.*?\]', r'\{.*?\}', r'<.*?>', r'TODO', r'PLACEHOLDER',
            r'Lorem ipsum', r'sample text', r'example content'
        ]
        
        for pattern in placeholder_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                patterns.append(f"Placeholder text detected: {pattern}")
        
        # Check for unusual formatting
        if text.count('\n\n') > len(text) / 100:  # Too many paragraph breaks
            patterns.append("Unusual paragraph formatting")
        
        # Check for repetitive structure
        lines = text.split('\n')
        if len(set(lines)) < len(lines) * 0.8:  # High line repetition
            patterns.append("Repetitive text structure")
        
        return patterns
    
    def analyze_pdf(self, pdf_path: str, filename: str) -> PDFAnalysisResult:
        """Complete PDF analysis pipeline"""
        start_time = time.time()
        
        try:
            # Extract content
            content = self.extract_text_and_metadata(pdf_path)
            text = content['text']
            metadata = content['metadata']
            page_count = content['page_count']
            
            # Analyze metadata
            metadata_inconsistencies = self.analyze_metadata_inconsistencies(metadata)
            
            # Analyze text for AI patterns
            text_analysis = self.detect_ai_language_patterns(text)
            
            # Calculate AI probability
            ai_probability = self.calculate_ai_probability(text_analysis, metadata_inconsistencies)
            
            # Detect suspicious patterns
            suspicious_patterns = self.detect_suspicious_patterns(text, metadata)
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            result = PDFAnalysisResult(
                ai_generated_probability=ai_probability,
                metadata_inconsistencies=metadata_inconsistencies,
                suspicious_patterns=suspicious_patterns,
                text_analysis=text_analysis,
                processing_time=processing_time
            )
            
            logger.info(f"PDF analysis completed: {filename} -> AI probability: {ai_probability:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"Error analyzing PDF {filename}: {e}")
            return PDFAnalysisResult(
                ai_generated_probability=0.0,
                metadata_inconsistencies=[f"Analysis error: {str(e)}"],
                suspicious_patterns=[],
                text_analysis={},
                processing_time=time.time() - start_time
            )

# Global service instance
pdf_analysis_service = PDFAnalysisService()