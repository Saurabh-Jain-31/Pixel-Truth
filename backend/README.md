# AI Authenticity Verification Platform - Backend

A production-ready backend system for detecting AI-generated content in images and PDFs using machine learning and metadata analysis.

## ğŸš€ Features

### Core Functionality
- **Image AI Detection**: CNN-based model to classify images as authentic, AI-generated, or manipulated
- **PDF Content Analysis**: Detect AI-generated text patterns and metadata inconsistencies
- **OSINT Metadata Analysis**: Extract and analyze EXIF data, detect suspicious patterns
- **Training Pipeline**: Extract datasets from archives and train custom models
- **Secure Authentication**: JWT-based auth with access/refresh tokens
- **User Management**: Role-based access (free/pro users)

### Technical Features
- **FastAPI Framework**: High-performance async API
- **MongoDB Database**: Scalable document storage
- **PyTorch ML Models**: CNN-based image classification
- **Archive Support**: ZIP, RAR, 7Z, TAR extraction for training data
- **Docker Ready**: Containerized deployment
- **Production Safe**: No hardcoded secrets, proper logging

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI (Python 3.10+)
- **ML Framework**: PyTorch
- **Database**: MongoDB
- **Image Processing**: OpenCV, PIL
- **PDF Processing**: PyMuPDF
- **Authentication**: JWT tokens
- **Archive Handling**: rarfile, py7zr
- **Deployment**: Docker

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry point
â”‚   â”œâ”€â”€ api/                 # API route handlers
â”‚   â”‚   â”œâ”€â”€ auth.py          # Authentication endpoints
â”‚   â”‚   â”œâ”€â”€ analysis.py      # Image/PDF analysis endpoints
â”‚   â”‚   â””â”€â”€ history.py       # Analysis history endpoints
â”‚   â”œâ”€â”€ models/              # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ user.py          # User models
â”‚   â”‚   â””â”€â”€ analysis.py      # Analysis result models
â”‚   â”œâ”€â”€ services/            # Business logic services
â”‚   â”‚   â”œâ”€â”€ auth.py          # Authentication service
â”‚   â”‚   â”œâ”€â”€ image_analysis.py # Image analysis service
â”‚   â”‚   â”œâ”€â”€ pdf_analysis.py  # PDF analysis service
â”‚   â”‚   â””â”€â”€ archive_extractor.py # Archive extraction service
â”‚   â”œâ”€â”€ utils/               # Utility functions
â”‚   â”‚   â””â”€â”€ file_handler.py  # File handling utilities
â”‚   â””â”€â”€ core/                # Core configuration
â”‚       â”œâ”€â”€ config.py        # Application settings
â”‚       â””â”€â”€ database.py      # Database connection
â”œâ”€â”€ ml/                      # Machine learning components
â”‚   â”œâ”€â”€ model.py             # CNN model definition
â”‚   â””â”€â”€ train.py             # Training script
â”œâ”€â”€ tests/                   # Test files
â”œâ”€â”€ Dockerfile               # Docker configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example            # Environment variables template
â””â”€â”€ README.md               # This file
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- MongoDB
- Docker (optional)

### Local Development Setup

1. **Clone and navigate to backend directory**
```bash
git clone <repository>
cd backend
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your configuration
```

5. **Start MongoDB**
```bash
# Using Docker
docker run -d -p 27017:27017 --name mongodb mongo:latest

# Or install MongoDB locally
```

6. **Run the application**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

### Docker Deployment

1. **Build Docker image**
```bash
docker build -t ai-verification-backend .
```

2. **Run with Docker Compose** (create docker-compose.yml)
```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MONGODB_URL=mongodb://mongodb:27017
      - SECRET_KEY=your-secret-key
    depends_on:
      - mongodb
  
  mongodb:
    image: mongo:latest
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db

volumes:
  mongodb_data:
```

```bash
docker-compose up -d
```

## ğŸ“š API Documentation

Once running, visit:
- **Interactive API Docs**: `http://localhost:8000/docs`
- **ReDoc Documentation**: `http://localhost:8000/redoc`

### Key Endpoints

#### Authentication
- `POST /auth/register` - Register new user
- `POST /auth/login` - Login and get tokens
- `POST /auth/refresh` - Refresh access token
- `GET /auth/me` - Get current user info

#### Analysis
- `POST /analyze/image` - Analyze image for AI detection
- `POST /analyze/pdf` - Analyze PDF for AI-generated content
- `POST /analyze/dataset/upload` - Upload training dataset archive
- `GET /analyze/datasets` - List available datasets

#### History
- `GET /history` - Get analysis history with pagination
- `GET /history/image/{id}` - Get detailed image analysis
- `GET /history/pdf/{id}` - Get detailed PDF analysis
- `DELETE /history/image/{id}` - Delete image analysis
- `DELETE /history/pdf/{id}` - Delete PDF analysis

## ğŸ¤– Training Custom Models

### Prepare Training Data

1. **Organize your dataset** in this structure:
```
dataset/
â”œâ”€â”€ authentic/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ai_generated/
â”‚   â”œâ”€â”€ ai_image1.jpg
â”‚   â”œâ”€â”€ ai_image2.png
â”‚   â””â”€â”€ ...
â””â”€â”€ manipulated/
    â”œâ”€â”€ edited1.jpg
    â”œâ”€â”€ edited2.png
    â””â”€â”€ ...
```

2. **Create archive** (ZIP, RAR, 7Z, or TAR)
```bash
zip -r my_dataset.zip dataset/
```

3. **Upload via API** (Pro users only)
```bash
curl -X POST "http://localhost:8000/analyze/dataset/upload" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "file=@my_dataset.zip" \
  -F "name=my_custom_dataset" \
  -F "description=Custom dataset for AI detection"
```

### Train Model

```bash
cd ml
python train.py --dataset my_custom_dataset --epochs 50 --batch_size 32
```

Or extract from archive directly:
```bash
python train.py --archive /path/to/dataset.zip --dataset my_dataset --epochs 50
```

### Training Features
- **Automatic data splitting** (80% train, 10% val, 10% test)
- **Data augmentation** (rotation, flip, color jitter)
- **Transfer learning** with ResNet50
- **Early stopping** and learning rate scheduling
- **Comprehensive evaluation** with confusion matrix
- **Training visualization** plots

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DEBUG` | Enable debug mode | `False` |
| `SECRET_KEY` | JWT secret key | Required |
| `MONGODB_URL` | MongoDB connection string | `mongodb://localhost:27017` |
| `DATABASE_NAME` | Database name | `ai_verification_db` |
| `MAX_FILE_SIZE` | Max upload size (bytes) | `52428800` (50MB) |
| `ACCESS_TOKEN_EXPIRE_MINUTES` | Token expiry | `30` |

### Model Configuration

- **Image Size**: 224x224 pixels
- **Batch Size**: 32 (configurable)
- **Architecture**: ResNet50 + custom classifier
- **Classes**: authentic, ai_generated, manipulated

## ğŸš€ Deployment

### Railway Deployment

1. **Connect GitHub repository** to Railway
2. **Set environment variables**:
```
SECRET_KEY=your-generated-secret-key
MONGODB_URL=mongodb+srv://user:pass@cluster.mongodb.net/dbname
DEBUG=False
```
3. **Deploy automatically** on push

### Render Deployment

1. **Create new Web Service** from GitHub
2. **Build Command**: `pip install -r requirements.txt`
3. **Start Command**: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`
4. **Set environment variables** in dashboard

### AWS/GCP Deployment

Use the provided Dockerfile with your preferred container service:
- AWS ECS/Fargate
- Google Cloud Run
- Azure Container Instances

## ğŸ§ª Testing

```bash
# Install test dependencies
pip install pytest pytest-asyncio httpx

# Run tests
pytest tests/

# Run with coverage
pytest --cov=app tests/
```

## ğŸ“Š Monitoring and Logging

The application includes:
- **Structured logging** with timestamps
- **API usage tracking** in database
- **Health check endpoints** (`/health`)
- **Error handling** with proper HTTP status codes
- **Request/response logging**

## ğŸ”’ Security Features

- **JWT Authentication** with access/refresh tokens
- **Password hashing** with bcrypt
- **File type validation** and size limits
- **CORS configuration**
- **Input validation** with Pydantic
- **SQL injection prevention** (NoSQL)
- **No hardcoded secrets**

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:
- Create an issue in the repository
- Check the API documentation at `/docs`
- Review the logs for error details

## ğŸ”„ Version History

- **v1.0.0** - Initial release with core functionality
  - Image AI detection
  - PDF content analysis
  - User authentication
  - Training pipeline
  - Archive extraction
  - Docker deployment